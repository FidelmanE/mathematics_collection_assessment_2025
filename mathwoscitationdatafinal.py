# -*- coding: utf-8 -*-
"""MathWoSCitationDataFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y4VTss9DMdVlwvJyKxnLFVMadBbUNeWj
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive/')

drive.mount("/content/drive/", force_remount=True)

!ls

WoSdata = pd.read_excel('/content/drive/My Drive/Data Services/Collections Assessments/Math2025WOSData/MathWoSPublications10yrs.xlsx')

WoSdata.columns

WoSdata = WoSdata.rename(columns={'CR: Author, year, source, volume, page, DOI;': 'cited_references'})

WoSdata['cited_references'] = WoSdata['cited_references'].astype(str)

WoSdata = WoSdata.assign(cited_references=WoSdata['cited_references'].str.split(';')).explode('cited_references')

WoSdata['whole_reference'] = WoSdata['cited_references'].copy()

import re

def extract_year(text):
  """Extracts the four numeric digits after the first comma and space."""
  match = re.search(r",\s*(\d{4})", text)
  if match:
    return match.group(1)
  else:
    return None

WoSdata['pubyear'] = WoSdata['cited_references'].apply(extract_year)

import re

def extract_text_after_doi(text):
  match = re.search(r"DOI\s*(.+)", text)
  if match:
    return match.group(1).strip()
  else:
    return None

WoSdata['cited_references'] = WoSdata['cited_references'].apply(extract_text_after_doi)
WoSdata = WoSdata.dropna(subset=['cited_references'])

def clean_cited_references(text):
    """
    Cleans the cited references by splitting on commas,
    keeping the last part, and removing the "DOI " prefix if present.
    """
    # Split on commas and keep the last part (after potential "DOI ")
    parts = text.split(",")
    last_part = parts[-1].strip()

    # Remove "DOI " if present at the beginning
    if last_part.startswith("DOI "):
        last_part = last_part[4:]  # Remove the first 4 characters ("DOI ")

    return last_part

# Apply the clean_cited_references function using .loc
WoSdata.loc[:, 'cited_references'] = WoSdata['cited_references'].apply(clean_cited_references)

def clean_cited_references(text):
    """
    Cleans the cited references by splitting on commas,
    keeping the last part, and removing the "DOI " prefix if present.
    """
    # Split on commas and keep the last part (after potential "DOI ")
    parts = text.split(",")
    last_part = parts[-1].strip()

    # Remove "DOI " if present at the beginning
    if last_part.startswith("DOI "):
        last_part = last_part[4:]  # Remove the first 4 characters ("DOI ")

    return last_part

# Apply the clean_cited_references function using .loc
WoSdata.loc[:, 'cited_references'] = WoSdata['cited_references'].apply(clean_cited_references)

WoSdata.loc[:, 'crossref_url'] = 'https://api.crossref.org/works/' + WoSdata['cited_references'].str.rstrip(']').astype(str)

WoSdata.loc[:, 'crossref_url'] = WoSdata['crossref_url'].str.strip()

WoSdata['crossref_url'].sample(30)

len(WoSdata)

import requests
import re
import time

def get_issn(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()

        # Delay to avoid overwhelming server (adjust as needed)
        #time.sleep(1)  # Wait for 1 second before next request

        issns = []  # List to store all ISSNs found

        # Check existing methods first
        if 'ISSN' in data['message'] and isinstance(data['message']['ISSN'], list):
            issns.extend(data['message']['ISSN'])  # Add all ISSNs from the 'ISSN' key

        if 'issn-type' in data['message'] and isinstance(data['message']['issn-type'], list):
            for issn_type in data['message']['issn-type']:
                if 'value' in issn_type:
                    issns.append(issn_type['value'])  # Add ISSN from 'issn-type'

        # New method: Extract from "link" array using regex
        if 'link' in data['message'] and isinstance(data['message']['link'], list):
            for link_obj in data['message']['link']:
                if 'URL' in link_obj:
                    match = re.search(r"/ISSN\s*([0-9-]+)", link_obj['URL'])
                    if match:
                        issns.append(match.group(1))  # Add ISSN from link URL

        # New method: Extract from brackets after "ISSN:"
        if 'reference' in data['message']:
            for ref in data['message']['reference']:
                if 'key' in ref and ref['key'] == 'issn':
                    issns.append(ref['value'])  # Add ISSN from reference

        # Return all ISSNs as a comma-separated string
        return ', '.join(issns) if issns else None

    except (requests.exceptions.RequestException, KeyError, IndexError, TypeError) as e:
        print(f"Error fetching ISSN from URL {url}: {e}")
        return None

WoSdata['ISSN'] = WoSdata['crossref_url'].apply(get_issn)

# Split ISSN values into separate rows
WoSdata_split = WoSdata.assign(ISSN=WoSdata['ISSN'].str.split(', ')).explode('ISSN')

# Group by 'crossref_url' and keep only unique ISSNs
# Filter out None values before applying unique and join
WoSdata_unique_issn = WoSdata_split.groupby('crossref_url', as_index=False)['ISSN'].agg(lambda x: ', '.join(x.dropna().unique()))

# Merge the unique ISSNs back into the original DataFrame (optional)
WoSdata = WoSdata.merge(WoSdata_unique_issn, on='crossref_url', suffixes=('', '_unique'))
WoSdata = WoSdata.drop('ISSN', axis=1)  # Remove the original ISSN column
WoSdata = WoSdata.rename(columns={'ISSN_unique': 'ISSN'})

import requests
import re
import time

def get_isbn(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()

        # Delay to avoid overwhelming server (adjust as needed)
        #time.sleep(1)  # Wait for 1 second before next request

        isbns = []  # List to store all ISBNs found

        # Check for 'ISBN' key and extract the first value
        if 'ISBN' in data['message'] and isinstance(data['message']['ISBN'], list) and len(data['message']['ISBN']) > 0:
            return data['message']['ISBN'][0]

        # If ISBN not found directly, check within 'isbn-type'
        if 'isbn-type' in data['message'] and isinstance(data['message']['isbn-type'], list):
            for isbn_type in data['message']['isbn-type']:
                if 'value' in isbn_type:
                    isbns.append(isbn_type['value'])

        # Check for partial matches in links for ISBN (similar to ISSN logic)
        if 'link' in data['message'] and isinstance(data['message']['link'], list):
            for link_obj in data['message']['link']:
                if 'URL' in link_obj:
                    match = re.search(r"/ISBN\s*([0-9-]+)", link_obj['URL'])
                    if match:
                        isbns.append(match.group(1))

        # Check for ISBN in references (similar to ISSN logic)
        if 'reference' in data['message']:
            for ref in data['message']['reference']:
                if 'key' in ref and ref['key'].lower() == 'isbn':
                    isbns.append(ref['value'])

        # Return the first ISBN found or None
        return isbns[0] if isbns else None

    except (requests.exceptions.RequestException, KeyError, IndexError, TypeError) as e:
        print(f"Error fetching ISBN from URL {url}: {e}")
        return None

# Apply the function and store the results in a new column called "ISBN"
WoSdata['ISBN'] = WoSdata['crossref_url'].apply(get_isbn)

KBART = pd.read_table('/content/drive/My Drive/Data Services/RAPIDElectronic/2025/Mar/Kbart.txt', header=0, delimiter='\t', low_memory=False, na_values=['', 'NaN', 'nan'])

KBART['online_identifier'] = KBART['online_identifier'].astype(str)
KBART['online_identifier'] = KBART['online_identifier'].replace(r'\.0$', '', regex=True)
KBART['online_identifier'] = KBART['online_identifier'].str.strip()
KBART['print_identifier'] = KBART['print_identifier'].astype(str)
KBART['print_identifier'] = KBART['print_identifier'].replace(r'\.0$', '', regex=True)
KBART['print_identifier'] = KBART['print_identifier'].str.strip()

KBART['date_last_issue_online'] = KBART['date_last_issue_online'].astype(str)

import numpy as np

KBART['date_last_issue_online'] = KBART['date_last_issue_online'].fillna('9999').replace({'nan': '9999', '<NA>': '9999'})

KBART['date_last_issue_online'] = KBART['date_last_issue_online'].astype(str).str[:4]
KBART['date_first_issue_online'] = KBART['date_first_issue_online'].astype(str).str[:4]

import sqlite3
conn = sqlite3.connect("MathWOSData2025.db")
cursor = conn.cursor()
cursor.execute("DROP TABLE IF EXISTS MathWOSData2025")

KBART.to_sql('KBART', conn, if_exists='replace', index=False)

WoSdata = WoSdata[['whole_reference', 'pubyear', 'cited_references', 'ISSN', 'ISBN', 'AF', 'TI', 'SO']]

import pandas as pd

WoSdata_split = WoSdata.assign(ISSN=WoSdata['ISSN'].str.split(', ')).explode('ISSN')

WoSdata_unique_issn = WoSdata_split.groupby('cited_references', as_index=False)['ISSN'].agg(lambda x: ', '.join(x.dropna().unique()))

# Merge the unique ISSNs back into the original DataFrame
WoSdata = WoSdata.merge(WoSdata_unique_issn, on='cited_references', suffixes=('', '_unique'))
# Remove the original ISSN column and rename the new one
WoSdata = WoSdata.drop('ISSN', axis=1)
WoSdata = WoSdata.rename(columns={'ISSN_unique': 'ISSN'})

# Clean up ISSN values with multiple commas by keeping only the first one
def clean_issn(issn_str):
  if issn_str:
    return issn_str.split(',')[0].strip()  # Keep only the first ISSN
  else:
    return None

WoSdata['ISSN'] = WoSdata['ISSN'].apply(clean_issn)

WoSdata = pd.read_excel('/content/drive/My Drive/Data Services/Collections Assessments/Math2025WOSData/WosDataManualCleaning.xlsx')

WoSdata.to_sql('WoSdata', conn, if_exists='replace', index=False)

"""Overlap on Holdings"""

cursor.execute("CREATE INDEX idx_WoSdata_ISSN ON WoSdata (ISSN);")
cursor.execute("CREATE INDEX idx_WoSdata_ISBN ON WoSdata (ISBN);")
cursor.execute("CREATE INDEX idx_KBART_online_identifier ON KBART (online_identifier);")
cursor.execute("CREATE INDEX idx_KBART_print_identifier ON KBART (print_identifier);")
conn.commit()

KBART = pd.read_sql("""SELECT * FROM KBART WHERE [coverage_depth] In ('fulltext', 'ebook')""", conn)
KBART.to_sql('KBART', conn, if_exists='replace', index=False)

Overlap = pd.read_sql(
    """
    SELECT DISTINCT T1.whole_reference, T1.pubyear, T1.cited_references, T1.ISSN, T1.ISBN, T1.AF, T1.TI, T1.SO, T2.date_first_issue_online, T2.date_last_issue_online
    FROM WoSdata AS T1
    INNER JOIN KBART AS T2 ON (T1.[ISSN] = T2.[online_identifier] OR T1.[ISSN] = T2.[print_identifier] OR T1.[ISBN] = T2.[online_identifier] OR T1.[ISBN] = T2.[print_identifier])
    WHERE (T1.[ISSN] IS NOT NULL OR T1.[ISBN] IS NOT NULL)
    AND CAST(T1.pubyear AS INTEGER) BETWEEN CAST(SUBSTR(T2.date_first_issue_online, 1, 4) AS INTEGER) AND CAST(SUBSTR(T2.date_last_issue_online, 1, 4) AS INTEGER)
    """,
    conn)

Overlap.to_excel('/content/drive/My Drive/Data Services/Collections Assessments/Math2025WOSData/Overlap.xlsx')

"""No ISSN or ISBN, manual lookup"""

NoWoSdataISSNorISBN = pd.read_sql("""SELECT * FROM WoSdata WHERE WoSdata.[ISSN] Is Null AND WoSdata.[ISBN] Is Null""", conn)

NoWoSdataISSNorISBN.to_excel('/content/drive/My Drive/Data Services/Collections Assessments/Math2025WOSData/NoWoSdataISSNorISBN.xlsx')

"""Overlap on ISSN or ISBN but not holdings"""

OverlapOnTitleOnly = pd.read_sql(
    """
    SELECT DISTINCT T1.whole_reference, T1.pubyear, T1.cited_references, T1.ISSN, T1.ISBN, T1.AF, T1.TI, T1.SO, T2.date_first_issue_online, T2.date_last_issue_online
    FROM WoSdata AS T1
    INNER JOIN KBART AS T2 ON (T1.[ISSN] = T2.[online_identifier] OR T1.[ISSN] = T2.[print_identifier] OR T1.[ISBN] = T2.[online_identifier] OR T1.[ISBN] = T2.[print_identifier])
    WHERE (T1.[ISSN] IS NOT NULL OR T1.[ISBN] IS NOT NULL)
    AND CAST(T1.pubyear AS INTEGER) NOT BETWEEN CAST(SUBSTR(T2.date_first_issue_online, 1, 4) AS INTEGER) AND CAST(SUBSTR(T2.date_last_issue_online, 1, 4) AS INTEGER)
    """,
    conn)

OverlapOnTitleOnly.to_excel('/content/drive/My Drive/Data Services/Collections Assessments/Math2025WOSData/OverlapOnTitleOnly.xlsx')

NoMatchOnTitle = pd.read_sql(
    """
    SELECT DISTINCT
    T1.whole_reference,
    T1.pubyear,
    T1.cited_references,
    T1.ISSN,
    T1.ISBN,
    T1.AF,
    T1.TI,
    T1.SO
FROM
    WoSdata AS T1
WHERE
    (T1.ISSN IS NOT NULL OR T1.ISBN IS NOT NULL)
    AND NOT EXISTS (
        SELECT
            1
        FROM
            KBART AS T2
        WHERE
            (T1.ISSN = T2.online_identifier OR T1.ISSN = T2.print_identifier OR T1.ISBN = T2.online_identifier OR T1.ISBN = T2.print_identifier)
    );
    """,
    conn)

NoMatchOnTitle.to_excel('/content/drive/My Drive/Data Services/Collections Assessments/Math2025WOSData/NoMatchOnTitle.xlsx')

"""Crosscheck!"""

OverlapOnTitleOnly = OverlapOnTitleOnly.drop_duplicates(subset=['whole_reference'], keep='first')
Overlap = Overlap.drop_duplicates(subset=['whole_reference'], keep='first')
NoWoSdataISSNorISBN = NoWoSdataISSNorISBN.drop_duplicates(subset=['whole_reference'], keep='first')
NoMatchOnTitle = NoMatchOnTitle.drop_duplicates(subset=['whole_reference'], keep='first')

XCheck= pd.concat([OverlapOnTitleOnly, Overlap, NoWoSdataISSNorISBN, NoMatchOnTitle])

# Identify duplicates based on 'whole_reference'
duplicates = XCheck.duplicated(subset=['whole_reference'], keep='first')

# Drop duplicates, keeping the first occurrence
XCheck_deduped = XCheck[~duplicates]

# Save the deduplicated table back to the database (optional)
XCheck_deduped.to_sql('XCheck_deduped', conn, if_exists='replace', index=False)

WoSdata.to_sql('WoSdata', conn, if_exists='replace', index=False)

WhatisInWoSDataButNotQueryOutput = pd.read_sql("""SELECT * FROM WoSdata WHERE WoSdata.whole_reference NOT IN (SELECT XCheck_deduped.whole_reference FROM XCheck_deduped)""", conn)

print(WhatisInWoSDataButNotQueryOutput)

# Identify duplicates based on 'whole_reference'
duplicates = WoSdata.duplicated(subset=['whole_reference'], keep='first')

# Drop duplicates, keeping the first occurrence
WoSdata_deduped = WoSdata[~duplicates]

len(WoSdata_deduped)